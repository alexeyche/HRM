#!/usr/bin/env python3
"""
Simplified test script for program synthesis input/output processing

This script tests the core data processing components without the full HRM model.
"""

import os
import json
import torch
import numpy as np
from typing import Dict, Any

from models.program_synthesis_processor import ProgramSynthesisProcessor, ProgramSynthesisDataset
from dataset.build_program_synthesis_dataset import PROGRAM_TEMPLATES, ASTSimplifier


def create_test_examples() -> list[Dict[str, Any]]:
    """Create test examples for program synthesis"""
    examples = []
    
    # Use a few simple templates from the dataset
    test_templates = ['sum_up_to_n', 'max_of_two', 'absolute_value', 'is_even']
    
    for name in test_templates:
        if name in PROGRAM_TEMPLATES:
            template = PROGRAM_TEMPLATES[name]
            example = {
                'specification': {
                    'name': name,
                    'description': template['description'],
                    'inputs': template['inputs'],
                    'outputs': template['outputs'],
                    'examples': template['base_examples'][:5]  # Use first 5 examples
                },
                'implementation': template['implementation']
            }
            examples.append(example)
    
    return examples


def test_ast_simplifier():
    """Test the ASTSimplifier"""
    print("Testing ASTSimplifier...")
    
    simplifier = ASTSimplifier(max_nodes=30, max_edges=25)
    
    # Test simple function
    code = """def program(n):
    return n * 2"""
    
    # Test sparse representation
    sparse_ast = simplifier.ast_to_sparse_representation(code)
    print(f"Sparse AST representation:")
    for key, value in sparse_ast.items():
        if isinstance(value, np.ndarray):
            print(f"  {key}: shape {value.shape}, dtype {value.dtype}")
        else:
            print(f"  {key}: {value}")
    
    # Test flattened representation
    flattened = simplifier.flatten_sparse_ast(sparse_ast)
    print(f"Flattened AST: shape {flattened.shape}, dtype {flattened.dtype}")
    print(f"First 10 elements: {flattened[:10]}")
    
    # Test human-readable JSON
    readable_ast = simplifier.ast_to_simplified_json(code)
    print(f"Human-readable AST:")
    print(json.dumps(readable_ast, indent=2))
    
    print("‚úì ASTSimplifier tests passed\n")


def test_processor():
    """Test the ProgramSynthesisProcessor"""
    print("Testing ProgramSynthesisProcessor...")
    
    processor = ProgramSynthesisProcessor(
        max_examples=5,
        max_spec_tokens=64,
        max_nodes=30,
        max_edges=25,
        vocab_size=1024
    )
    
    # Test specification encoding
    test_spec = {
        'name': 'sum_up_to_n',
        'description': 'Sum up all numbers up to N',
        'inputs': [{'type': 'int', 'description': 'The input number N'}],
        'outputs': [{'type': 'int', 'description': 'The sum'}],
        'examples': [
            {'input': 5, 'output': 15},
            {'input': 10, 'output': 55}
        ]
    }
    
    encoded = processor.encode_specification(test_spec)
    print(f"Encoded specification shape: {encoded.shape}")
    print(f"First 10 tokens: {encoded[:10].tolist()}")
    
    # Decode some tokens to verify
    decoded_tokens = []
    for i in range(min(15, len(encoded))):
        token_id = encoded[i].item()
        if token_id in processor.id_to_token:
            decoded_tokens.append(processor.id_to_token[token_id])
        else:
            decoded_tokens.append(f"<UNK_{token_id}>")
    print(f"Decoded tokens: {decoded_tokens}")
    
    # Test AST processing
    implementation = """def program(n):
    return sum(range(1, n + 1))"""
    
    ast_data = processor.process_ast_target(implementation)
    print(f"AST processing results:")
    for key, value in ast_data.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: shape {value.shape}, dtype {value.dtype}")
        else:
            print(f"  {key}: {value}")
    
    # Check valid nodes
    valid_nodes = torch.where(ast_data['node_exists'])[0]
    print(f"Valid nodes: {len(valid_nodes)}")
    if len(valid_nodes) > 0:
        print(f"Node types for valid nodes: {ast_data['node_types'][valid_nodes]}")
        print(f"Node values for valid nodes: {ast_data['node_values'][valid_nodes]}")
    
    # Test batch creation
    examples = create_test_examples()
    batch = processor.create_hrm_batch(examples[:2])
    print(f"Batch creation results:")
    for key, value in batch.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: shape {value.shape}, dtype {value.dtype}")
        elif isinstance(value, dict):
            print(f"  {key}:")
            for subkey, subvalue in value.items():
                if isinstance(subvalue, torch.Tensor):
                    print(f"    {subkey}: shape {subvalue.shape}, dtype {subvalue.dtype}")
    
    # Test AST decoding
    decoded_ast = processor.decode_ast_output(
        ast_data['node_exists'],
        ast_data['adjacency'],
        ast_data['node_types'],
        ast_data['node_values']
    )
    print(f"Decoded AST:")
    print(f"  Num nodes: {decoded_ast['num_nodes']}")
    print(f"  Num edges: {len(decoded_ast['edges'])}")
    for i, node in enumerate(decoded_ast['nodes'][:5]):
        print(f"  Node {i}: {node['type_name']} (value: {node['value']})")
    
    print("‚úì ProgramSynthesisProcessor tests passed\n")


def test_dataset():
    """Test dataset loading and processing"""
    print("Testing ProgramSynthesisDataset...")
    
    # Create temporary test data
    test_data_dir = "/tmp/test_program_synthesis"
    os.makedirs(test_data_dir, exist_ok=True)
    
    test_file = os.path.join(test_data_dir, "test_data.json")
    test_examples = create_test_examples()
    
    with open(test_file, 'w') as f:
        json.dump(test_examples, f, indent=2)
    
    # Test dataset loading
    processor = ProgramSynthesisProcessor()
    dataset = ProgramSynthesisDataset(
        data_path=test_file,
        processor=processor,
        max_examples=100
    )
    
    print(f"Dataset loaded with {len(dataset)} examples")
    
    # Test individual access
    example = dataset[0]
    print(f"First example keys: {list(example.keys())}")
    print(f"Specification name: {example['specification']['name']}")
    
    # Test batch creation
    batch = dataset.get_random_batch(batch_size=2)
    print(f"Random batch created with keys: {list(batch.keys())}")
    
    # Verify batch contents
    print(f"Input specs shape: {batch['inputs'].shape}")
    print(f"AST targets keys: {list(batch['ast_targets'].keys())}")
    
    # Clean up
    os.remove(test_file)
    os.rmdir(test_data_dir)
    
    print("‚úì ProgramSynthesisDataset tests passed\n")


def test_loss_computation():
    """Test loss computation logic"""
    print("Testing loss computation...")
    
    from models.program_synthesis_processor import create_program_synthesis_loss
    
    # Create mock targets and predictions
    batch_size, max_nodes = 2, 10
    
    ast_targets = {
        'node_exists': torch.randint(0, 2, (batch_size, max_nodes), dtype=torch.bool),
        'node_types': torch.randint(0, 30, (batch_size, max_nodes)),
        'adjacency': torch.randint(0, 2, (batch_size, max_nodes, max_nodes), dtype=torch.bool),
        'node_values': torch.randint(-10, 10, (batch_size, max_nodes))
    }
    
    ast_predictions = {
        'node_exists': torch.randn(batch_size, max_nodes),  # Logits
        'node_types': torch.randn(batch_size, max_nodes, 30),  # Logits 
        'adjacency': torch.randn(batch_size, max_nodes, max_nodes),  # Logits
        'node_values': torch.randn(batch_size, max_nodes)  # Predictions
    }
    
    loss = create_program_synthesis_loss(ast_targets, ast_predictions)
    print(f"Combined loss: {loss.item():.4f}")
    print(f"Loss requires grad: {loss.requires_grad}")
    
    print("‚úì Loss computation tests passed\n")


def test_complex_ast():
    """Test more complex AST processing"""
    print("Testing complex AST processing...")
    
    # Test factorial function (has recursion)
    factorial_code = """def program(n):
    if n <= 1:
        return 1
    return n * program(n - 1)"""
    
    simplifier = ASTSimplifier(max_nodes=30, max_edges=25)
    
    try:
        sparse_ast = simplifier.ast_to_sparse_representation(factorial_code)
        print(f"Factorial AST nodes: {sparse_ast['num_nodes']}")
        print(f"Factorial AST edges: {len(sparse_ast['edge_list'])}")
        
        readable_ast = simplifier.ast_to_simplified_json(factorial_code)
        print(f"Factorial function structure:")
        print(json.dumps(readable_ast, indent=2)[:500] + "...")  # First 500 chars
        
    except Exception as e:
        print(f"Note: Complex AST processing failed (expected): {e}")
    
    # Test simpler control flow
    if_code = """def program(n):
    if n > 0:
        return n
    else:
        return -n"""
    
    sparse_ast = simplifier.ast_to_sparse_representation(if_code)
    print(f"If-else AST nodes: {sparse_ast['num_nodes']}")
    print(f"If-else AST edges: {len(sparse_ast['edge_list'])}")
    
    print("‚úì Complex AST tests passed\n")


def main():
    """Run all tests"""
    print("Running Program Synthesis Input/Output Processing Tests\n")
    
    try:
        test_ast_simplifier()
        test_processor()
        test_dataset()
        test_loss_computation()
        test_complex_ast()
        
        print("üéâ All tests passed successfully!")
        print("\nKey components implemented:")
        print("‚úì Multi-modal specification encoding (types, examples, descriptions)")
        print("‚úì AST graph representation processing") 
        print("‚úì Program synthesis dataset integration")
        print("‚úì Loss computation for multi-component training")
        print("‚úì Sparse and efficient AST representations")
        
    except Exception as e:
        print(f"‚ùå Test failed with error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())